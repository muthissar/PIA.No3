{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from CIA.getters import get_handler, get_data_processor, \\\n",
    "    get_dataloader_generator, get_decoder, get_positional_embedding, get_sos_embedding\n",
    "import time\n",
    "import importlib\n",
    "from CIA.positional_embeddings.positional_embedding import PositionalEmbedding\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import einops\n",
    "from CIA.utils import get_free_port\n",
    "from CIA.data_processors.data_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 3\n",
    "rank = GPU\n",
    "world_size = 1\n",
    "rank = 0\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(get_free_port())\n",
    "if torch.cuda.is_available() and GPU in list(range(4)):\n",
    "    torch.distributed.init_process_group(backend=\"nccl\", world_size=world_size, rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    device_ids = [rank]\n",
    "    output_device = rank\n",
    "    device = f\"cuda:{rank}\"\n",
    "else:\n",
    "    # cpu case\n",
    "    torch.distributed.init_process_group(backend=\"gloo\", world_size=world_size, rank=rank)\n",
    "    device_ids = None\n",
    "    output_device = None\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index dictionnary\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "Loading models CausalEventsDecoderFullCat\n",
      "Load over-fitted model\n",
      "Placeholder duration: tensor([25.], device='cuda:0')\n",
      "End of decoding due to END symbol generation\n",
      "num events gen: 483 - done: True - decoding end: 173\n",
      "Time of generated sequence 22.979997634887695\n",
      "File out/inpainted_seq.mid written\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'models/piano_event_performer_2021-10-01_16:03:06'\n",
    "# config =  importlib.import_module('CIA.configs.piarceiverStack').config\n",
    "# NOTE: override configuration\n",
    "config = importlib.import_module('.config_autoreg', f'{model_dir.replace(\"/\", \".\")}').config\n",
    "for t in ['time_dilation', 'velocity_shift', 'transposition']:\n",
    "    config['dataloader_generator_kwargs']['transformations'][t] = False\n",
    "config['data_processor_kwargs']['num_events_before'] = 1\n",
    "config['data_processor_kwargs']['num_events_after'] = 0\n",
    "\n",
    "load = True\n",
    "overfitted = True\n",
    "\n",
    "# === Decoder ====\n",
    "# dataloader generator\n",
    "dataloader_generator = get_dataloader_generator(\n",
    "    dataset=config[\"dataset\"],\n",
    "    dataloader_generator_kwargs=config[\"dataloader_generator_kwargs\"],\n",
    ")\n",
    "\n",
    "# data processor\n",
    "data_processor: DataProcessor = get_data_processor(\n",
    "    dataloader_generator=dataloader_generator,\n",
    "    data_processor_type=config[\"data_processor_type\"],\n",
    "    data_processor_kwargs=config[\"data_processor_kwargs\"],\n",
    ")\n",
    "\n",
    "# positional embedding\n",
    "positional_embedding: PositionalEmbedding = get_positional_embedding(\n",
    "    dataloader_generator=dataloader_generator,\n",
    "    positional_embedding_dict=config[\"positional_embedding_dict\"],\n",
    ")\n",
    "\n",
    "# sos embedding\n",
    "sos_embedding = get_sos_embedding(\n",
    "    dataloader_generator=dataloader_generator,\n",
    "    sos_embedding_dict=config[\"sos_embedding_dict\"],\n",
    ")\n",
    "\n",
    "decoder = get_decoder(\n",
    "    data_processor=data_processor,\n",
    "    dataloader_generator=dataloader_generator,\n",
    "    positional_embedding=positional_embedding,\n",
    "    sos_embedding=sos_embedding,\n",
    "    decoder_kwargs=config[\"decoder_kwargs\"],\n",
    "    training_phase=False,\n",
    "    handler_type=config[\"handler_type\"],\n",
    ")\n",
    "\n",
    "decoder.to(device)\n",
    "decoder = DistributedDataParallel(\n",
    "    module=decoder,\n",
    "    device_ids=[rank],\n",
    "    output_device=rank\n",
    "    # )\n",
    "    ,\n",
    "    find_unused_parameters=True,\n",
    ")\n",
    "\n",
    "decoder_handler = get_handler(\n",
    "    handler_type=config[\"handler_type\"],\n",
    "    decoder=decoder,\n",
    "    model_dir=model_dir,\n",
    "    dataloader_generator=dataloader_generator,\n",
    ")\n",
    "\n",
    "if load:\n",
    "    if overfitted:\n",
    "        decoder_handler.load(early_stopped=False)\n",
    "    else:\n",
    "        decoder_handler.load(early_stopped=True)\n",
    "\n",
    "if hasattr(decoder_handler.model.module.transformer, \"fix_projection_matrices_\"):\n",
    "    decoder_handler.model.module.transformer.fix_projection_matrices_()\n",
    "\n",
    "# null_superconditioning = [1, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "# (_, generator_val, _) = dataloader_generator.dataloaders(\n",
    "#     batch_size=1, num_workers=num_workers, shuffle_val=True\n",
    "# )\n",
    "# original_x = next(generator_val)[\"x\"]\n",
    "piece = 'Bach, Carl Philipp Emanuel, Keyboard Sonata in F major, H.55, zRPTf8eGLWc'\n",
    "# {-63, -53, ..., -3, 7, ... }\n",
    "start_time = -3\n",
    "for split, samples in  dataloader_generator.dataset.list_ids.items():\n",
    "    for id_, sample in enumerate(samples):\n",
    "        if piece == sample['score_name'] and start_time == sample['start_time']:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "dataloader_generator.dataset.split = split\n",
    "data = dataloader_generator.dataset[id_]\n",
    "original_x = torch.stack([data[e] for e in dataloader_generator.features], dim=-1)[None]\n",
    "# NOTE: num_events_middle likely the number of notes which will be created? Actually more events are created...\n",
    "# NOTE: first event is always the same, probably because it's a special start token, .\n",
    "# NOTE: Num events, most likely, defines the event (after decode start) that should be how long time (in the original score) we should we should generate. \n",
    "# TODO: what's the difference between num_events_middle and num_max_generated_events? Probably \n",
    "# Only needed for setting up start_decode and end_decode.\n",
    "# Does the model actually actually not attend to the after (future) tokens while decoding? \n",
    "# Actually preprocess should return (shuffle the data): \n",
    "# before  middle  after -> before  placeholder  after  SOD (start of decoding)  middle  END XX XX (pad)\n",
    "# NOTE: metadata_dict['decoding_end'] is not used for inpaint_non_optimized \n",
    "num_events_middle = 10\n",
    "num_max_generated_events = 480\n",
    "# x, metadata_dict = data_processor.preprocess(original_x, num_events_middle=num_events_middle)\n",
    "# \"open ended\"\n",
    "secs_dec = 25.\n",
    "batch_size = 1\n",
    "placeholder, placeholder_duration_token = data_processor.compute_placeholder(placeholder_duration=torch.tensor([secs_dec]), batch_size=batch_size)\n",
    "before = einops.repeat(data_processor.start_tokens, 'd ->b 1 d', b=batch_size)\n",
    "after = torch.empty(batch_size, 0, placeholder.shape[-1], dtype=torch.long, device=device)\n",
    "sod =  einops.repeat(data_processor.start_tokens, 'd ->b 1 d', b=batch_size)\n",
    "query = torch.cat([before, placeholder, after, sod], dim=1)\n",
    "middle = torch.zeros(batch_size, num_events_middle-1, placeholder.shape[-1], dtype=torch.long, device=device)\n",
    "end = einops.repeat(data_processor.end_tokens, 'd ->b 1 d', b=batch_size)\n",
    "padding_len = config['dataloader_generator_kwargs']['sequences_size'] - query.shape[1] - middle.shape[1] - 1\n",
    "padding = einops.repeat(data_processor.end_tokens, 'd ->b padding d', b=batch_size, padding=padding_len)\n",
    "x = torch.cat([\n",
    "    query,\n",
    "    middle,\n",
    "    end, \n",
    "    padding,\n",
    "\n",
    "],\n",
    "axis=1)\n",
    "metadata_dict = {\n",
    "    'placeholder_duration': torch.tensor([secs_dec], device=device),\n",
    "    'decoding_start': query.shape[1],\n",
    "    # 'decoding_end': query.shape[1] - 1+  num_events_middle,\n",
    "}\n",
    "\n",
    "# NOTE: with current setup this corresponds to open ended generation.\n",
    "# [START]  placeholder (programs the model to decode until a certain total time, where the end note is produced)  []  SOD  [m0,m1,...,m9]  END XX XX\n",
    "# NOTE: loss_mask is not used in inpainting, how do we handle attention mask?\n",
    "# metadata_dict.pop('loss_mask')\n",
    "# metadata_dict['decoding_start'] = 0\n",
    "# metadata_dict['decoding_end'] = 1024\n",
    "start_time = time.time()\n",
    "# NOTE: Here it always attends (!autoregressively!) to full sequence but updates during (autoregressive) decoding.\n",
    "# NOTE: model decodes from decode start and decode up to the next num_events_middle. \n",
    "# If at one time the placeholder_duration is exceeded, or END symbol, then it terminates with done.\n",
    "# Otherwise it terminates after resampling all events in \"middle\".\n",
    "\n",
    "# \n",
    "#  If it did not . We can probably fix this \n",
    "# by giving a sequence of some mask token, from the beginning. \n",
    "(\n",
    "    x_gen,\n",
    "    generated_region,\n",
    "    decoding_end,\n",
    "    num_event_generated,\n",
    "    done,\n",
    ") = decoder_handler.inpaint_non_optimized(\n",
    "    x=x.clone(), metadata_dict=metadata_dict, temperature=1.0, top_p=0.95, top_k=0, num_max_generated_events=num_max_generated_events\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "x_inpainted = torch.cat([before, generated_region, after, end], axis=1)\n",
    "# x_inpainted = data_processor.postprocess(x_gen, decoding_end, metadata_dict)\n",
    "# x_inpainted_orig = data_processor.postprocess(original_x, decoding_end, metadata_dict)\n",
    "print(f\"Time of generated sequence {dataloader_generator.get_elapsed_time(x_inpainted[0][None])[0,-1]}\")\n",
    "# dataloader_generator.write(x_inpainted_orig[0], 'out/orig')\n",
    "dataloader_generator.write(x_inpainted[0], 'out/inpainted_seq')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
